# Neural Network from Scratch: XOR Problem

This is a learning project focused on understanding the core concepts of neural networks rather than using pre-built training methods from frameworks like TensorFlow or PyTorch.


## ğŸ”§ Implementation Details

**XOR Truth Table:**
```
Input (x1, x2) | Output
---------------|-------
(0, 0)         | 0
(0, 1)         | 1
(1, 0)         | 1
(1, 1)         | 0
```

## ğŸ—ï¸ Architecture

- **Input Layer:** 2 neurons (x1, x2)
- **Hidden Layer:** 2 neurons with sigmoid activation
- **Output Layer:** 1 neuron with sigmoid activation

## âœ¨ Features

- âœ… Forward propagation
- âœ… Backpropagation algorithm
- âœ… Binary cross-entropy loss function
- âœ… Manual weight and bias updates
- âœ… No high-level ML frameworks (only NumPy for basic operations)

## ğŸš€ Quick Start

### Prerequisites

- Python 3.7+
- NumPy


### Run

```bash
python3 xor.py
```

## ğŸ“Š Example Output

```
Testing different weight initializations...
âœ“ Seed 456 achieves 100% accuracy

Using seed 456 for full training

[Epoch     0] âœ— Accuracy: 2/4 (50%) Avg Loss: 0.7298
[Epoch  1000] âœ“ Accuracy: 4/4 (100%) Avg Loss: 0.0108

âœ“ Perfect accuracy achieved at epoch 1000!

============================================================
TRAINING COMPLETE - Final Parameters:
============================================================
Hidden Layer 1: w11= -5.060 w12= -5.059 b1=  7.506
Hidden Layer 2: w21= -7.023 w22= -7.137 b2=  2.930
Output Layer:   v1= 10.543 v2=-10.949 b3= -4.902
============================================================

============================================================
TEST RESULTS:
============================================================
âœ“ XOR(0,0) = 0 â†’ Predicted: 0 (prob: 0.0085)
âœ“ XOR(0,1) = 1 â†’ Predicted: 1 (prob: 0.9904)
âœ“ XOR(1,0) = 1 â†’ Predicted: 1 (prob: 0.9902)
âœ“ XOR(1,1) = 0 â†’ Predicted: 0 (prob: 0.0150)
============================================================
Accuracy: 4/4 (100%)
============================================================
```

## ğŸ“š Learning Objectives

This project was created to understand neural networks at the most fundamental level by:

- Implementing perceptrons and multi-layer networks manually
- Understanding forward and backward propagation
- Learning gradient descent and weight updates
- Avoiding high-level abstractions to grasp the underlying mathematics

## ğŸ”§ Implementation Details

- **Activation Function:** Sigmoid
- **Loss Function:** Binary Cross-Entropy
- **Optimization:** Gradient Descent
- **Learning Rate:** 0.5
- **Epochs:** 30,000 (with early stopping)
- **Initialization:** Tests multiple random seeds to find optimal starting weights
